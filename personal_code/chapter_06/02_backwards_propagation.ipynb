{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_img(img: np.ndarray, padding: int) -> np.ndarray:\n",
    "    # Get size of image\n",
    "    num_rows = img.shape[0]\n",
    "    num_cols = img.shape[1]\n",
    "    \n",
    "    # Create padding for rows\n",
    "    zero_rows = np.zeros((padding, num_cols))\n",
    "    # Add padding to each side of the image\n",
    "    img = np.vstack((zero_rows, img))\n",
    "    img = np.vstack((img, zero_rows))\n",
    "    \n",
    "    # Create padding for columns (need to add 2*padding\n",
    "    # as the image is now wider)\n",
    "    zero_cols = np.zeros((num_rows+2*padding, padding))\n",
    "    # Add padding to top and bottom of the image\n",
    "    img = np.hstack((zero_cols, img))\n",
    "    img = np.hstack((img, zero_cols))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolute_2d(img: np.ndarray, kernel: np.ndarray, padding: int) -> np.ndarray:\n",
    "    # Assume square mask\n",
    "    kernel_size = kernel.shape[0]\n",
    "    img_pad = pad_img(img, padding)\n",
    "    num_rows = img_pad.shape[0]\n",
    "    num_cols = img_pad.shape[1]\n",
    "    # Want to make sure that the output is the size of\n",
    "    # the image without padding\n",
    "    img_conv = np.zeros((num_rows-2*padding, num_cols-2*padding))\n",
    "    \n",
    "    # The +1 is needed to get the center to be the first pixel\n",
    "    # The padding gives half of it but as it is odd you need to add 1\n",
    "    for i in range(num_rows - kernel_size + 1):\n",
    "        for j in range(num_cols - kernel_size + 1):\n",
    "            img_window = img_pad[i:i+kernel_size, j:j+kernel_size]\n",
    "            img_conv[i, j] = (img_window.flatten()*kernel.flatten()).sum()\n",
    "    return img_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_relu(img: np.ndarray) -> np.ndarray:\n",
    "    # ReLu leaves positives the same, but sets negatives to 0\n",
    "    img[img<0] = 0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_convolution(img: np.ndarray) -> np.ndarray:\n",
    "    num_rows = img.shape[0]\n",
    "    num_cols=img.shape[1]\n",
    "    max_pool = np.zeros((int(num_rows/2), int(num_cols/2)))\n",
    "    # Shift the window across in steps of 2\n",
    "    for i in range(0, num_rows, 2):\n",
    "        for j in range(0, num_cols, 2):\n",
    "            # Get a window of the image\n",
    "            img_window = img[i:i+2, j:j+2]\n",
    "            # Take the maximum value of the window and\n",
    "            # set it as the value in teh max pool\n",
    "            max_pool[int(i/2), int(j/2)] = img_window.max()\n",
    "    return max_pool\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_sigmoid(flattened: np.ndarray,\n",
    "                       weights: np.ndarray,\n",
    "                       bias: float) -> float:\n",
    "    # Get the activation value\n",
    "    x = weights.dot(flattened) + bias\n",
    "    # This is just the sigmoid function\n",
    "    y_hat = 1/(1+np.exp(-x))\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(img: np.ndarray,\n",
    "                 kernel: np.ndarray,\n",
    "                 bias: float,\n",
    "                 weights: np.ndarray,\n",
    "                 bias_flat:float) -> np.ndarray:\n",
    "    # Get padding size\n",
    "    padding = int(kernel.shape[0]/2)\n",
    "    # Convolute image\n",
    "    img_conv = convolute_2d(img, kernel, padding)\n",
    "    # Add bias term and ReLu activation\n",
    "    img_conv = img_conv + bias\n",
    "    img_conv = activation_relu(img_conv)\n",
    "    # Max pool the image to shrink it, then flatten\n",
    "    img_pool = pool_convolution(img_conv)\n",
    "    flattened_pool = img_pool.flatten()\n",
    "    # Get y_hat\n",
    "    y_hat = activation_sigmoid(flattened_pool, weights, bias_flat)\n",
    "    return img_conv, flattened_pool, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_wrt_weights(y_hat: float,\n",
    "                             y: int,\n",
    "                             flattened: np.ndarray) -> np.ndarray:\n",
    "    # Set up shape for d_weights\n",
    "    d_weights = np.squeeze(np.zeros((1, len(flattened))))\n",
    "    # Calculate the derivative\n",
    "    d = (y_hat - y)*y_hat*(1-y_hat)\n",
    "    # Multiply the derivative by the flattened pool\n",
    "    for i in range(len(flattened)):\n",
    "        d_weights[i] = d*flattened[i]\n",
    "    return d_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_wrt_flattened(y_hat: float,\n",
    "                               y: int,\n",
    "                               weights: np.ndarray) -> np.ndarray:\n",
    "    # Set up shape for d_flattened\n",
    "    d_flattened = np.squeeze(np.zeros((1, len(weights))))\n",
    "    # Calculate the derivative\n",
    "    d = (y_hat - y)*y_hat*(1-y_hat)\n",
    "    # Multiply the derivative by the weights\n",
    "    for i in range(len(weights)):\n",
    "        d_flattened[i] = d*weights[i]\n",
    "    return d_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_wrt_bias_flat(y_hat: float,\n",
    "                               y: int) -> float:\n",
    "    # Simple as bias is 1x1 array\n",
    "    d_bias_flat = (y_hat - y)*y_hat*(1-y_hat)\n",
    "    return d_bias_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_wrt_pooled(d_bias_flat: np.ndarray) -> np.ndarray:\n",
    "    # Get shape of the pooled matrix (before it was flattened)\n",
    "    # Square root as it is a square\n",
    "    num_col_rows = int(len(d_bias_flat)**0.5)\n",
    "    # Reshape it to a square\n",
    "    d_pooled = d_bias_flat.reshape((num_col_rows, num_col_rows))\n",
    "    return d_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_wrt_convoluted(d_pooled: np.ndarray,\n",
    "                                img_conv: np.ndarray) -> np.ndarray:\n",
    "    num_rows = img_conv.shape[0]\n",
    "    num_cols = img_conv.shape[1]\n",
    "    # Set up shape for derivative\n",
    "    d_img_conv = np.zeros((num_rows, num_cols))\n",
    "    # Work backwards from pooling to get each window with steps of 2\n",
    "    for i in range(0, num_rows, 2):\n",
    "        for j in range(0, num_cols, 2):\n",
    "            img_conv_window = img_conv[i:i+2, j:j+2]\n",
    "            # Get the index of the maximum of img_conv_window \n",
    "            # to place the deriviative\n",
    "            # All other values were ignored in .max() so will be zero\n",
    "            idx = np.unravel_index(np.argmax(\n",
    "                img_conv_window, axis=None), img_conv_window.shape)\n",
    "            d_img_conv[i+idx[0], j+idx[1]] = d_pooled[int(i/2), int(j/2)]\n",
    "    return d_img_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain_rule_gradients(img_conv: np.ndarray,\n",
    "                             d_img_conv: np.ndarray,\n",
    "                             img: np.ndarray,\n",
    "                             u: int, v: int) -> float:\n",
    "    # u and v are the row and column positions in the kernel\n",
    "    d_kernel_u_v = 0\n",
    "    for i in range(img_conv.shape[0]):\n",
    "        for j in range(img_conv.shape[1]):\n",
    "            # Check that the item is positive and is a\n",
    "            # valid positon in the image\n",
    "            if (img_conv[i, j] > 0 and i - u > 0 and j - v > 0\n",
    "                and i - u < img.shape[0] and j - v < img.shape[1]):\n",
    "                d_kernel_u_v = (d_kernel_u_v +\n",
    "                                img[i -u, j - v]*d_img_conv[i, j])\n",
    "    return d_kernel_u_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_wrt_kernel(img_conv: np.ndarray,\n",
    "                            img: np.ndarray,\n",
    "                            y_hat: float,\n",
    "                            y: int,\n",
    "                            weights: np.ndarray,\n",
    "                            kernel: np.ndarray) -> np.ndarray:\n",
    "    d_flattened = get_gradient_wrt_flattened(y_hat, y, weights)\n",
    "    d_pooled = get_gradient_wrt_pooled(d_flattened)\n",
    "    d_img_conv = get_gradient_wrt_convoluted(d_pooled, img_conv)\n",
    "    kernel_shape = kernel.shape[0]\n",
    "    # As the kernel is 0 centered need to find how many squares\n",
    "    # either side of the center to get it's relvative indices as\n",
    "    # they are (0, 0) at top left\n",
    "    kernel_either_side = int((kernel_shape-1)/2)\n",
    "    d_kernel = np.zeros((kernel_shape, kernel_shape))\n",
    "    # The plus 1 is needed as upper limit is ignored\n",
    "    for u in range(0-kernel_either_side, kernel_either_side+1):\n",
    "        for v in range(0-kernel_either_side, kernel_either_side+1):\n",
    "            d_kernel[u+kernel_either_side, v+kernel_either_side\n",
    "                     ] = get_chain_rule_gradients(\n",
    "                         img_conv, d_img_conv, img, u, v)\n",
    "    return d_kernel, d_img_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version will be vectorized instead of relying on loops\n",
    "def get_gradient_wrt_bias(img_conv: np.ndarray,\n",
    "                          d_img_conv: np.ndarray) -> np.ndarray:\n",
    "    d_bias = d_img_conv[img_conv > 0].sum()\n",
    "    return d_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation_pass(img: np.ndarray,\n",
    "                          img_conv: np.ndarray,\n",
    "                          flattened: np.ndarray,\n",
    "                          weights: np.ndarray,\n",
    "                          kernel: np.ndarray,\n",
    "                          y_hat: float,\n",
    "                          y: int) -> np.ndarray:\n",
    "    # Combine all of the backward gradient steps into a\n",
    "    # single function\n",
    "    d_weights = get_gradient_wrt_weights(y_hat, y, flattened)\n",
    "    d_bias_flat = get_gradient_wrt_bias_flat(y_hat, y)\n",
    "    d_kernel, d_img_conv = get_gradient_wrt_kernel(\n",
    "        img_conv, img, y_hat, y, weights, kernel)\n",
    "    d_bias = get_gradient_wrt_bias(img_conv, d_img_conv)\n",
    "    return d_kernel, d_bias, d_weights, d_bias_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    # Initialise the base state\n",
    "    # Keep the values small as large values do not work well\n",
    "    kernel = 0.01*np.random.randn(5, 5)\n",
    "    bias = 0.01*np.random.randn()\n",
    "    # Remove the extra 1 dimension from this array\n",
    "    weights = np.squeeze(0.01*np.random.randn(1,256))\n",
    "    bias_flat = 0.01*np.random.randn()\n",
    "    return kernel, bias, weights, bias_flat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5130462625739269\n",
      "0.45872157021088356\n",
      "0.3809208747677658\n",
      "0.3073262590739291\n",
      "0.20696897709892656\n"
     ]
    }
   ],
   "source": [
    "# Create arbitraty image and label\n",
    "img = np.random.randint(1, 255, (32, 32))\n",
    "y = 0\n",
    "\n",
    "# Initialise paramaters randomly\n",
    "kernel, bias, weights, bias_flat  = init_params()\n",
    "\n",
    "for i in range(5):\n",
    "    # Perform a forward pass\n",
    "    img_conv, flattened, y_hat = forward_pass(img, kernel, bias, weights, bias_flat)\n",
    "\n",
    "    print(y_hat)\n",
    "\n",
    "    # Perform a back propagation pass to get weight updates\n",
    "    d_kernel, d_bias, d_weights, d_bias_flat = back_propagation_pass(\n",
    "        img, img_conv, flattened, weights, kernel, y_hat, y)\n",
    "\n",
    "    # Use a learning rate and update weights\n",
    "    alpha = 0.001\n",
    "    kernel = kernel - alpha * d_kernel\n",
    "    bias = bias - alpha * d_bias\n",
    "    weights = weights - alpha * d_weights\n",
    "    bias_flat = bias_flat - alpha * d_bias_flat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
